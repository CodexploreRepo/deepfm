{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Model Layers Guide\n",
    "\n",
    "This notebook explores the four core layers that sit on top of `FeatureEmbedding`:\n",
    "**FM**, **DNN**, **CIN**, and **Multi-Head Self-Attention**.\n",
    "Each layer consumes a different view of the embedding output and captures a different type of feature interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 0. Setup — Create a Synthetic Schema & Embeddings\n",
    "\n",
    "We'll use a small synthetic schema so this notebook runs instantly without downloading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from deepfm.data.schema import FieldSchema, DatasetSchema, FeatureType\n",
    "from deepfm.models.layers.embedding import FeatureEmbedding\n",
    "\n",
    "# Define a small schema: 6 fields, mix of vocab sizes and embed dims\n",
    "fields = {\n",
    "    \"user_id\":    FieldSchema(\"user_id\",    FeatureType.SPARSE, vocabulary_size=100, embedding_dim=16),\n",
    "    \"item_id\":    FieldSchema(\"item_id\",    FeatureType.SPARSE, vocabulary_size=200, embedding_dim=16),\n",
    "    \"gender\":     FieldSchema(\"gender\",     FeatureType.SPARSE, vocabulary_size=3,   embedding_dim=4),\n",
    "    \"age\":        FieldSchema(\"age\",        FeatureType.SPARSE, vocabulary_size=8,   embedding_dim=4),\n",
    "    \"occupation\": FieldSchema(\"occupation\", FeatureType.SPARSE, vocabulary_size=22,  embedding_dim=8),\n",
    "    \"city\":       FieldSchema(\"city\",       FeatureType.SPARSE, vocabulary_size=50,  embedding_dim=8),\n",
    "}\n",
    "schema = DatasetSchema(fields=fields, label_field=\"label\")\n",
    "FM_DIM = 16\n",
    "\n",
    "# Build embedding layer and get the three outputs\n",
    "emb = FeatureEmbedding(schema, fm_embed_dim=FM_DIM)\n",
    "emb.eval()\n",
    "\n",
    "batch = {\n",
    "    \"user_id\":    torch.randint(1, 100, (8,)),\n",
    "    \"item_id\":    torch.randint(1, 200, (8,)),\n",
    "    \"gender\":     torch.randint(1, 3,   (8,)),\n",
    "    \"age\":        torch.randint(1, 8,   (8,)),\n",
    "    \"occupation\": torch.randint(1, 22,  (8,)),\n",
    "    \"city\":       torch.randint(1, 50,  (8,)),\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    first_order, field_embeddings, flat_embeddings = emb(batch)\n",
    "\n",
    "B = field_embeddings.size(0)\n",
    "F = field_embeddings.size(1)\n",
    "D = field_embeddings.size(2)\n",
    "\n",
    "print(f\"Batch size B={B}, Fields F={F}, FM dim D={D}\")\n",
    "print(f\"first_order:      {first_order.shape}\")\n",
    "print(f\"field_embeddings: {field_embeddings.shape}\")\n",
    "print(f\"flat_embeddings:  {flat_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. FM Interaction Layer\n",
    "\n",
    "The FM layer computes **second-order feature interactions** efficiently in O(F*D) using the identity:\n",
    "\n",
    "```\n",
    "sum_{i<j} <v_i, v_j> = 0.5 * ( (sum_i v_i)^2 - sum_i (v_i^2) )\n",
    "```\n",
    "\n",
    "This avoids the O(F^2 * D) cost of explicit pairwise dot products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepfm.models.layers.fm import FMInteraction\n",
    "\n",
    "fm = FMInteraction()\n",
    "print(fm)\n",
    "print(f\"\\nParameters: {sum(p.numel() for p in fm.parameters())} (none — FM is parameter-free!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    fm_output = fm(field_embeddings)\n",
    "\n",
    "print(f\"Input:  field_embeddings {field_embeddings.shape}  — (B, F, D)\")\n",
    "print(f\"Output: fm_output        {fm_output.shape}         — (B, 1)\")\n",
    "print(f\"\\nFM interaction values (one scalar per sample):\")\n",
    "print(fm_output.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### Verifying the Math\n",
    "\n",
    "Let's confirm the efficient formula matches the explicit pairwise computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicit O(F^2) pairwise computation for verification\n",
    "with torch.no_grad():\n",
    "    explicit = torch.zeros(B, 1)\n",
    "    for i in range(F):\n",
    "        for j in range(i + 1, F):\n",
    "            dot = (field_embeddings[:, i] * field_embeddings[:, j]).sum(dim=1, keepdim=True)\n",
    "            explicit += dot\n",
    "\n",
    "print(\"Efficient vs Explicit (should match):\")\n",
    "print(f\"  Max difference: {(fm_output - explicit).abs().max().item():.2e}\")\n",
    "print(f\"  All close:      {torch.allclose(fm_output, explicit, atol=1e-5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. DNN Layer\n",
    "\n",
    "The DNN is a standard MLP that processes the **flat concatenated embeddings**.\n",
    "It captures arbitrary higher-order interactions through non-linear transformations.\n",
    "\n",
    "Stack: `Linear → (BatchNorm) → Activation → Dropout` repeated per hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepfm.models.layers.dnn import DNN\n",
    "\n",
    "input_dim = flat_embeddings.shape[1]\n",
    "dnn = DNN(\n",
    "    input_dim=input_dim,\n",
    "    hidden_units=[128, 64, 32],\n",
    "    activation=\"relu\",\n",
    "    dropout=0.1,\n",
    "    use_batch_norm=True,\n",
    ")\n",
    "print(dnn)\n",
    "print(f\"\\nOutput dim: {dnn.output_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    dnn.eval()\n",
    "    dnn_output = dnn(flat_embeddings)\n",
    "\n",
    "print(f\"Input:  flat_embeddings {flat_embeddings.shape}  — (B, total_dim)\")\n",
    "print(f\"Output: dnn_output      {dnn_output.shape}       — (B, last_hidden)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### Layer-by-Layer Dimension Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i, module in enumerate(dnn.mlp):\n",
    "    name = module.__class__.__name__\n",
    "    params = sum(p.numel() for p in module.parameters())\n",
    "    if hasattr(module, \"in_features\"):\n",
    "        detail = f\"{module.in_features} -> {module.out_features}\"\n",
    "    elif hasattr(module, \"num_features\"):\n",
    "        detail = f\"features={module.num_features}\"\n",
    "    elif hasattr(module, \"p\"):\n",
    "        detail = f\"p={module.p}\"\n",
    "    else:\n",
    "        detail = \"-\"\n",
    "    rows.append({\"idx\": i, \"layer\": name, \"detail\": detail, \"params\": params})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(f\"Total DNN parameters: {sum(p.numel() for p in dnn.parameters()):,}\\n\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Comparing Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for act_name in [\"relu\", \"leaky_relu\", \"gelu\", \"tanh\"]:\n",
    "    d = DNN(input_dim, [64, 32], activation=act_name, dropout=0.0, use_batch_norm=False)\n",
    "    d.eval()\n",
    "    with torch.no_grad():\n",
    "        out = d(flat_embeddings)\n",
    "    rows.append({\n",
    "        \"activation\": act_name,\n",
    "        \"output_mean\": f\"{out.mean().item():.4f}\",\n",
    "        \"output_std\": f\"{out.std().item():.4f}\",\n",
    "        \"pct_zero\": f\"{(out == 0).float().mean().item():.1%}\",\n",
    "    })\n",
    "\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. CIN Layer (Compressed Interaction Network)\n",
    "\n",
    "CIN is the key innovation in **xDeepFM**. It captures **explicit, vector-wise** higher-order interactions\n",
    "(unlike FM which is scalar-wise, and DNN which is implicit).\n",
    "\n",
    "Each CIN layer:\n",
    "1. Computes an outer product between the current hidden state and the original input\n",
    "2. Compresses with Conv1d (kernel_size=1) — like a learned weighted sum of interaction maps\n",
    "3. Optionally splits: half feeds forward, half goes to the output pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepfm.models.layers.cin import CIN\n",
    "\n",
    "cin = CIN(num_fields=F, embed_dim=D, layer_sizes=[128, 128], split_half=True)\n",
    "print(cin)\n",
    "print(f\"\\nOutput dim: {cin.output_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    cin_output = cin(field_embeddings)\n",
    "\n",
    "print(f\"Input:  field_embeddings {field_embeddings.shape}  — (B, F, D)\")\n",
    "print(f\"Output: cin_output       {cin_output.shape}       — (B, output_dim)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "### Understanding split_half\n",
    "\n",
    "With `split_half=True`, each intermediate layer splits its feature maps:\n",
    "- One half feeds into the **next** CIN layer (for deeper interactions)\n",
    "- The other half goes directly to the **output pool** (for shallower interactions)\n",
    "\n",
    "This gives the model multi-granularity: the output contains both 2nd-order and higher-order interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i, (conv, ds, ns) in enumerate(zip(cin.conv_layers, cin.direct_sizes, cin.next_sizes)):\n",
    "    rows.append({\n",
    "        \"layer\": i,\n",
    "        \"conv_in_channels\": conv.in_channels,\n",
    "        \"conv_out_channels\": conv.out_channels,\n",
    "        \"to_output_pool\": ds,\n",
    "        \"to_next_layer\": ns,\n",
    "        \"interaction_order\": i + 2,  # layer 0 = 2nd order, layer 1 = 3rd order, etc.\n",
    "    })\n",
    "\n",
    "print(f\"With split_half=True, output_dim = {cin.output_dim} = {' + '.join(str(d) for d in cin.direct_sizes)}\")\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "### Effect of split_half on Output Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [True, False]:\n",
    "    c = CIN(num_fields=F, embed_dim=D, layer_sizes=[128, 128], split_half=split)\n",
    "    with torch.no_grad():\n",
    "        out = c(field_embeddings)\n",
    "    params = sum(p.numel() for p in c.parameters())\n",
    "    print(f\"split_half={str(split):5s}  output_dim={c.output_dim:4d}  output_shape={out.shape}  params={params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Multi-Head Self-Attention Layer\n",
    "\n",
    "The attention layer refines field embeddings by learning **which field pairs matter most**.\n",
    "Used in **AttentionDeepFM** to replace or augment FM's uniform pairwise interactions.\n",
    "\n",
    "Standard transformer-style: Q/K/V projections → scaled dot-product → multi-head → residual + LayerNorm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepfm.models.layers.attention import MultiHeadSelfAttention\n",
    "\n",
    "attn = MultiHeadSelfAttention(\n",
    "    embed_dim=D,\n",
    "    num_heads=4,\n",
    "    attention_dim=64,\n",
    "    num_layers=2,\n",
    "    use_residual=True,\n",
    ")\n",
    "print(attn)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in attn.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    attn.eval()\n",
    "    attn_output = attn(field_embeddings)\n",
    "\n",
    "print(f\"Input:  field_embeddings {field_embeddings.shape}  — (B, F, D)\")\n",
    "print(f\"Output: attn_output      {attn_output.shape}      — (B, F, D)  (same shape!)\")\n",
    "print(f\"\\nAttention preserves the (B, F, D) shape — it refines embeddings, not reduces them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### Visualizing Attention Weights\n",
    "\n",
    "Let's extract the attention weights from the first layer to see which fields attend to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Manually extract attention weights from the first layer, first head\n",
    "block = attn.layers[0]\n",
    "with torch.no_grad():\n",
    "    Q = block.W_q(field_embeddings)  # (B, F, attn_dim)\n",
    "    K = block.W_k(field_embeddings)\n",
    "    head_dim = block.head_dim\n",
    "    num_heads = block.num_heads\n",
    "\n",
    "    # Reshape to multi-head\n",
    "    Q = Q.view(B, F, num_heads, head_dim).transpose(1, 2)  # (B, H, F, hd)\n",
    "    K = K.view(B, F, num_heads, head_dim).transpose(1, 2)\n",
    "\n",
    "    # Attention weights for head 0, sample 0\n",
    "    scores = torch.matmul(Q[0, 0], K[0, 0].T) / math.sqrt(head_dim)  # (F, F)\n",
    "    weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "field_names = list(schema.fields.keys())\n",
    "print(\"Attention weights (head 0, sample 0):\")\n",
    "print(f\"{'':15s}\", \"  \".join(f\"{n:>10s}\" for n in field_names))\n",
    "for i, name in enumerate(field_names):\n",
    "    row = \"  \".join(f\"{weights[i, j].item():10.3f}\" for j in range(F))\n",
    "    print(f\"{name:15s} {row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "### Residual Connection Effect\n",
    "\n",
    "With `use_residual=True`, the output is `LayerNorm(attention_out + input)`. This means\n",
    "the model can learn to pass through the original embeddings when attention isn't helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    attn_res = MultiHeadSelfAttention(D, num_heads=4, attention_dim=64, num_layers=1, use_residual=True)\n",
    "    attn_no_res = MultiHeadSelfAttention(D, num_heads=4, attention_dim=64, num_layers=1, use_residual=False)\n",
    "    attn_res.eval()\n",
    "    attn_no_res.eval()\n",
    "\n",
    "    out_res = attn_res(field_embeddings)\n",
    "    out_no_res = attn_no_res(field_embeddings)\n",
    "\n",
    "    # How much does the output differ from input?\n",
    "    diff_res = (out_res - field_embeddings).norm() / field_embeddings.norm()\n",
    "    diff_no_res = (out_no_res - field_embeddings).norm() / field_embeddings.norm()\n",
    "\n",
    "print(f\"Relative change from input:\")\n",
    "print(f\"  With residual:    {diff_res.item():.4f}\")\n",
    "print(f\"  Without residual: {diff_no_res.item():.4f}\")\n",
    "print(f\"\\nResidual connections keep outputs closer to the input (easier to train).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Comparing All Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [\n",
    "    {\n",
    "        \"layer\": \"FMInteraction\",\n",
    "        \"input\": \"field_embeddings (B,F,D)\",\n",
    "        \"output_shape\": str(tuple(fm_output.shape)),\n",
    "        \"interaction_type\": \"2nd-order, explicit\",\n",
    "        \"params\": sum(p.numel() for p in fm.parameters()),\n",
    "        \"used_in\": \"DeepFM, AttentionDeepFM\",\n",
    "    },\n",
    "    {\n",
    "        \"layer\": \"DNN\",\n",
    "        \"input\": \"flat_embeddings (B,total_dim)\",\n",
    "        \"output_shape\": str(tuple(dnn_output.shape)),\n",
    "        \"interaction_type\": \"higher-order, implicit\",\n",
    "        \"params\": sum(p.numel() for p in dnn.parameters()),\n",
    "        \"used_in\": \"DeepFM, xDeepFM, AttentionDeepFM\",\n",
    "    },\n",
    "    {\n",
    "        \"layer\": \"CIN\",\n",
    "        \"input\": \"field_embeddings (B,F,D)\",\n",
    "        \"output_shape\": str(tuple(cin_output.shape)),\n",
    "        \"interaction_type\": \"higher-order, explicit (vector-wise)\",\n",
    "        \"params\": sum(p.numel() for p in cin.parameters()),\n",
    "        \"used_in\": \"xDeepFM\",\n",
    "    },\n",
    "    {\n",
    "        \"layer\": \"MultiHeadSelfAttention\",\n",
    "        \"input\": \"field_embeddings (B,F,D)\",\n",
    "        \"output_shape\": str(tuple(attn_output.shape)),\n",
    "        \"interaction_type\": \"adaptive pairwise weighting\",\n",
    "        \"params\": sum(p.numel() for p in attn.parameters()),\n",
    "        \"used_in\": \"AttentionDeepFM\",\n",
    "    },\n",
    "]\n",
    "\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## 6. How Models Compose These Layers\n",
    "\n",
    "```\n",
    "FeatureEmbedding(batch)\n",
    "    |\n",
    "    +-- first_order (B,1)\n",
    "    +-- field_embeddings (B,F,D)\n",
    "    +-- flat_embeddings (B,total_dim)\n",
    "\n",
    "\n",
    "DeepFM:                                  xDeepFM:                               AttentionDeepFM:\n",
    "  logit = first_order                      logit = first_order                    logit = first_order\n",
    "        + FM(field_emb)                          + Linear(CIN(field_emb))               + FM(field_emb)\n",
    "        + Linear(DNN(flat_emb))                  + Linear(DNN(flat_emb))                + Linear(DNN(cat(\n",
    "                                                                                            Attn(field_emb).flatten(),\n",
    "                                                                                            flat_emb)))\n",
    "\n",
    "\n",
    "Key differences:\n",
    "  - DeepFM:          FM (2nd order explicit) + DNN (higher order implicit)\n",
    "  - xDeepFM:         CIN (higher order explicit, vector-wise) + DNN (higher order implicit)\n",
    "  - AttentionDeepFM: FM + Attention-refined embeddings fed to DNN (learned interaction importance)\n",
    "```\n",
    "\n",
    "All three share the same `FeatureEmbedding` — the only difference is how they process its outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
