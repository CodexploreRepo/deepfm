{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeatureEmbedding Guide\n",
    "\n",
    "This notebook explores how `FeatureEmbedding` converts raw feature indices from the MovieLens dataset\n",
    "into the three tensor views consumed by all CTR models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load MovieLens Data & Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema has 7 fields, total_embedding_dim = 64\n"
     ]
    }
   ],
   "source": [
    "from deepfm.config import ExperimentConfig\n",
    "from deepfm.data.movielens import MovieLensAdapter\n",
    "\n",
    "config = ExperimentConfig()\n",
    "adapter = MovieLensAdapter(config.data)\n",
    "schema, train_ds, val_ds, test_ds = adapter.build()\n",
    "\n",
    "print(f\"Schema has {schema.num_fields} fields, total_embedding_dim = {schema.total_embedding_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Examine Per-Field Embedding Dimensions\n",
    "\n",
    "Each field has its own `embedding_dim` based on cardinality.\n",
    "The FM/CIN/Attention components need a **common dimension**, so each field\n",
    "gets a projection layer when its `embedding_dim != fm_embed_dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fm_embed_dim = 16 (common dimension for FM/CIN/Attention)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>type</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>embed_dim</th>\n",
       "      <th>needs_projection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_id</td>\n",
       "      <td>sparse</td>\n",
       "      <td>944</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>movie_id</td>\n",
       "      <td>sparse</td>\n",
       "      <td>1679</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>sparse</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age</td>\n",
       "      <td>sparse</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>occupation</td>\n",
       "      <td>sparse</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>zip_prefix</td>\n",
       "      <td>sparse</td>\n",
       "      <td>383</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>genres</td>\n",
       "      <td>sequence</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        field      type  vocab_size  embed_dim  needs_projection\n",
       "0     user_id    sparse         944         16             False\n",
       "1    movie_id    sparse        1679         16             False\n",
       "2      gender    sparse           3          4              True\n",
       "3         age    sparse           8          4              True\n",
       "4  occupation    sparse          22          8              True\n",
       "5  zip_prefix    sparse         383          8              True\n",
       "6      genres  sequence          20          8              True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fm_embed_dim = config.feature.fm_embed_dim\n",
    "print(f\"fm_embed_dim = {fm_embed_dim} (common dimension for FM/CIN/Attention)\\n\")\n",
    "\n",
    "rows = []\n",
    "for name, field in schema.fields.items():\n",
    "    rows.append({\n",
    "        \"field\": name,\n",
    "        \"type\": field.feature_type.value,\n",
    "        \"vocab_size\": field.vocabulary_size,\n",
    "        \"embed_dim\": field.embedding_dim,\n",
    "        \"needs_projection\": field.embedding_dim != fm_embed_dim,\n",
    "    })\n",
    "\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the FeatureEmbedding Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureEmbedding(\n",
      "  (second_order_embeddings): ModuleDict(\n",
      "    (user_id): Embedding(944, 16, padding_idx=0)\n",
      "    (movie_id): Embedding(1679, 16, padding_idx=0)\n",
      "    (gender): Embedding(3, 4, padding_idx=0)\n",
      "    (age): Embedding(8, 4, padding_idx=0)\n",
      "    (occupation): Embedding(22, 8, padding_idx=0)\n",
      "    (zip_prefix): Embedding(383, 8, padding_idx=0)\n",
      "    (genres): EmbeddingBag(20, 8, mode='mean', padding_idx=0)\n",
      "  )\n",
      "  (first_order_embeddings): ModuleDict(\n",
      "    (user_id): Embedding(944, 1, padding_idx=0)\n",
      "    (movie_id): Embedding(1679, 1, padding_idx=0)\n",
      "    (gender): Embedding(3, 1, padding_idx=0)\n",
      "    (age): Embedding(8, 1, padding_idx=0)\n",
      "    (occupation): Embedding(22, 1, padding_idx=0)\n",
      "    (zip_prefix): Embedding(383, 1, padding_idx=0)\n",
      "    (genres): EmbeddingBag(20, 1, mode='mean', padding_idx=0)\n",
      "  )\n",
      "  (projections): ModuleDict(\n",
      "    (gender): Linear(in_features=4, out_features=16, bias=False)\n",
      "    (age): Linear(in_features=4, out_features=16, bias=False)\n",
      "    (occupation): Linear(in_features=8, out_features=16, bias=False)\n",
      "    (zip_prefix): Linear(in_features=8, out_features=16, bias=False)\n",
      "    (genres): Linear(in_features=8, out_features=16, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from deepfm.models.layers.embedding import FeatureEmbedding\n",
    "\n",
    "emb = FeatureEmbedding(schema, fm_embed_dim=fm_embed_dim)\n",
    "print(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Forward Pass on a Real Batch\n",
    "\n",
    "Let's grab a batch from the training DataLoader and pass it through the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      "  user_id          shape=torch.Size([4])  dtype=torch.int64\n",
      "  movie_id         shape=torch.Size([4])  dtype=torch.int64\n",
      "  gender           shape=torch.Size([4])  dtype=torch.int64\n",
      "  age              shape=torch.Size([4])  dtype=torch.int64\n",
      "  occupation       shape=torch.Size([4])  dtype=torch.int64\n",
      "  zip_prefix       shape=torch.Size([4])  dtype=torch.int64\n",
      "  genres           shape=torch.Size([4, 6])  dtype=torch.int64\n",
      "  labels           shape=torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(train_ds, batch_size=4, shuffle=True)\n",
    "batch_features, batch_labels = next(iter(loader))\n",
    "\n",
    "print(\"Input batch:\")\n",
    "for name, tensor in batch_features.items():\n",
    "    print(f\"  {name:15s}  shape={str(tensor.shape):15s}  dtype={tensor.dtype}\")\n",
    "print(f\"  {'labels':15s}  shape={str(batch_labels.shape):15s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensors:\n",
      "  first_order:      torch.Size([4, 1])      — linear term (summed across fields)\n",
      "  field_embeddings: torch.Size([4, 7, 16])  — projected to common fm_dim, one per field\n",
      "  flat_embeddings:  torch.Size([4, 64])   — raw concat of all field embeddings\n"
     ]
    }
   ],
   "source": [
    "emb.eval()\n",
    "with torch.no_grad():\n",
    "    first_order, field_embeddings, flat_embeddings = emb(batch_features)\n",
    "\n",
    "print(\"Output tensors:\")\n",
    "print(f\"  first_order:      {first_order.shape}      — linear term (summed across fields)\")\n",
    "print(f\"  field_embeddings: {field_embeddings.shape}  — projected to common fm_dim, one per field\")\n",
    "print(f\"  flat_embeddings:  {flat_embeddings.shape}   — raw concat of all field embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding the Three Outputs\n",
    "\n",
    "### 5a. First Order — `(B, 1)`\n",
    "Each feature has a first-order embedding of dim 1 (i.e., a scalar weight per feature value).\n",
    "These are summed across all fields to produce the linear term `<w, x>` in FM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First order values (one scalar per sample):\n",
      "tensor([[-1.2366],\n",
      "        [-0.7426],\n",
      "        [-1.1634],\n",
      "        [-0.7224]])\n",
      "\n",
      "Think of this as: bias + w_user_id + w_movie_id + w_gender + ... for each sample\n"
     ]
    }
   ],
   "source": [
    "print(\"First order values (one scalar per sample):\")\n",
    "print(first_order)\n",
    "print(f\"\\nThink of this as: bias + w_user_id + w_movie_id + w_gender + ... for each sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Field Embeddings — `(B, num_fields, fm_embed_dim)`\n",
    "\n",
    "Each field's embedding is **projected** to the common `fm_embed_dim` so that\n",
    "FM can compute pairwise dot products between fields.\n",
    "Fields with `embedding_dim == fm_embed_dim` (like user_id, movie_id at dim=16) skip the projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "field_embeddings shape: torch.Size([4, 7, 16])\n",
      "  B=4, F=7, D=16\n",
      "\n",
      "Per-field projected embeddings for sample 0:\n",
      "  user_id          [[-0.025335164740681648, 0.0015823764260858297, 0.05761401727795601, -0.024023396894335747] ...]  (dim=16)\n",
      "  movie_id         [[0.004626244306564331, -0.05903911590576172, -0.012329183518886566, 0.04952222481369972] ...]  (dim=16)\n",
      "  gender           [[-0.14277571439743042, -0.6031725406646729, -0.06658773869276047, 0.04329908639192581] ...]  (dim=16)\n",
      "  age              [[-0.05736514925956726, -0.5844314098358154, -0.49363431334495544, 0.2917306125164032] ...]  (dim=16)\n",
      "  occupation       [[0.3453277349472046, 0.037104543298482895, 0.2612850069999695, -0.014424063265323639] ...]  (dim=16)\n",
      "  zip_prefix       [[-0.018386617302894592, 0.04026094079017639, 0.10642841458320618, -0.05326128751039505] ...]  (dim=16)\n",
      "  genres           [[-0.4363022446632385, 0.31358152627944946, -0.14919301867485046, 0.44798552989959717] ...]  (dim=16)\n"
     ]
    }
   ],
   "source": [
    "print(f\"field_embeddings shape: {field_embeddings.shape}\")\n",
    "print(f\"  B={field_embeddings.shape[0]}, F={field_embeddings.shape[1]}, D={field_embeddings.shape[2]}\")\n",
    "print()\n",
    "\n",
    "field_names = list(schema.fields.keys())\n",
    "print(\"Per-field projected embeddings for sample 0:\")\n",
    "for i, name in enumerate(field_names):\n",
    "    vec = field_embeddings[0, i]\n",
    "    print(f\"  {name:15s}  [{vec[:4].tolist()} ...]  (dim={vec.shape[0]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. Flat Embeddings — `(B, total_dim)`\n",
    "\n",
    "The **raw** (non-projected) embeddings concatenated across all fields.\n",
    "This is the input to the DNN component. The total dim equals `sum(field.embedding_dim for all fields)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat_embeddings shape: torch.Size([4, 64])\n",
      "  total_dim = 64\n",
      "\n",
      "Composition:\n",
      "  user_id          dim= 16  positions [0:16)\n",
      "  movie_id         dim= 16  positions [16:32)\n",
      "  gender           dim=  4  positions [32:36)\n",
      "  age              dim=  4  positions [36:40)\n",
      "  occupation       dim=  8  positions [40:48)\n",
      "  zip_prefix       dim=  8  positions [48:56)\n",
      "  genres           dim=  8  positions [56:64)\n",
      "  TOTAL            dim=64\n"
     ]
    }
   ],
   "source": [
    "print(f\"flat_embeddings shape: {flat_embeddings.shape}\")\n",
    "print(f\"  total_dim = {flat_embeddings.shape[1]}\")\n",
    "print()\n",
    "\n",
    "# Show how total_dim is composed\n",
    "offset = 0\n",
    "print(\"Composition:\")\n",
    "for name, field in schema.fields.items():\n",
    "    end = offset + field.embedding_dim\n",
    "    print(f\"  {name:15s}  dim={field.embedding_dim:3d}  positions [{offset}:{end})\")\n",
    "    offset = end\n",
    "print(f\"  {'TOTAL':15s}  dim={offset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. OOV / Padding Behavior\n",
    "\n",
    "Index 0 is reserved for unknown/OOV. The `padding_idx=0` ensures these contribute\n",
    "zero to both first-order and second-order embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All-OOV input:\n",
      "  first_order sum:      0.000000  (should be ~0)\n",
      "  field_embeddings sum: 0.000000  (should be ~0)\n",
      "  flat_embeddings sum:  0.000000  (should be ~0)\n"
     ]
    }
   ],
   "source": [
    "# Create a batch with index 0 (OOV) for user_id\n",
    "oov_batch = {name: torch.zeros(1, dtype=torch.long) for name in field_names}\n",
    "# Sequence field needs 2D\n",
    "oov_batch[\"genres\"] = torch.zeros(1, 6, dtype=torch.long)\n",
    "\n",
    "with torch.no_grad():\n",
    "    fo_oov, fe_oov, fl_oov = emb(oov_batch)\n",
    "\n",
    "print(\"All-OOV input:\")\n",
    "print(f\"  first_order sum:      {fo_oov.abs().sum().item():.6f}  (should be ~0)\")\n",
    "print(f\"  field_embeddings sum: {fe_oov.abs().sum().item():.6f}  (should be ~0)\")\n",
    "print(f\"  flat_embeddings sum:  {fl_oov.abs().sum().item():.6f}  (should be ~0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Projection Layers\n",
    "\n",
    "Fields with `embedding_dim != fm_embed_dim` get a `nn.Linear` projection.\n",
    "Let's see which fields have projections and their parameter counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projection layers (field_dim -> 16):\n",
      "  gender           4 -> 16  (64 params)\n",
      "  age              4 -> 16  (64 params)\n",
      "  occupation       8 -> 16  (128 params)\n",
      "  zip_prefix       8 -> 16  (128 params)\n",
      "  genres           8 -> 16  (128 params)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Projection layers (field_dim -> {fm_embed_dim}):\")\n",
    "for name, proj in emb.projections.items():\n",
    "    field_dim = schema.fields[name].embedding_dim\n",
    "    params = sum(p.numel() for p in proj.parameters())\n",
    "    print(f\"  {name:15s}  {field_dim} -> {fm_embed_dim}  ({params} params)\")\n",
    "\n",
    "if not emb.projections:\n",
    "    print(\"  (none — all fields already match fm_embed_dim)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Parameter Count Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:     48,983\n",
      "Trainable parameters: 48,983\n",
      "\n",
      "  second_order_embeddings           45,412 params\n",
      "  first_order_embeddings             3,059 params\n",
      "  projections                          512 params\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in emb.parameters())\n",
    "trainable_params = sum(p.numel() for p in emb.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters:     {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print()\n",
    "\n",
    "# Breakdown by component\n",
    "for component_name, module_dict in [\n",
    "    (\"second_order_embeddings\", emb.second_order_embeddings),\n",
    "    (\"first_order_embeddings\", emb.first_order_embeddings),\n",
    "    (\"projections\", emb.projections),\n",
    "]:\n",
    "    params = sum(p.numel() for p in module_dict.parameters())\n",
    "    print(f\"  {component_name:30s}  {params:>8,} params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. How Models Consume These Outputs\n",
    "\n",
    "```\n",
    "FeatureEmbedding(batch)\n",
    "    |\n",
    "    ├── first_order (B, 1)  ──────────────────────> FM linear term: y_linear = bias + first_order\n",
    "    │\n",
    "    ├── field_embeddings (B, F, fm_dim)  ─────────> FM 2nd order: 0.5*(square_of_sum - sum_of_squares)\n",
    "    │                                     ├──────> CIN: vector-wise interactions (xDeepFM)\n",
    "    │                                     └──────> Attention: field self-attention (AttentionDeepFM)\n",
    "    │\n",
    "    └── flat_embeddings (B, total_dim)  ──────────> DNN: MLP(flat_embeddings) → logit\n",
    "```\n",
    "\n",
    "Key insight: **FM and DNN share the same embedding vectors** (no separate wide features).\n",
    "The only difference is that FM gets the projected view while DNN gets the raw concatenated view."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
